{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from datetime import date\n",
    "import re\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pacmap import PaCMAP\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import arxiv\n",
    "from sklearn.neighbors import KNeighborsTransformer\n",
    "from IPython.display import clear_output\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "from torch import nn\n",
    "device = 'cpu'\n",
    "torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_of_interest = ['computer science', 'computational biology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     17
    ]
   },
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def get_embedding(model, tokenizer, text):   \n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    \n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    \n",
    "    return sentence_embeddings.numpy()\n",
    "\n",
    "\n",
    "def get_papers_after_date(last_date):\n",
    "    search = arxiv.Search(\n",
    "      query = \"computer science\",\n",
    "      max_results = 1000,\n",
    "      sort_by = arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "    \n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "    model = AutoModel.from_pretrained('allenai/longformer-base-4096')\n",
    "    \n",
    "    paper_data = []\n",
    "    \n",
    "    for result in search.results():\n",
    "        ## To be changed\n",
    "        paper_date = (result.published+timedelta(days=1)).date()\n",
    "        ##\n",
    "        \n",
    "        \n",
    "        if paper_date < last_date.date():\n",
    "            break\n",
    "        \n",
    "        category_vector = [result.categories[i] if i<len(result.categories) else \"Nafin\" for i in range(3)]\n",
    "        \n",
    "        \n",
    "        abstract = result.summary.replace('\\n',' ').replace('\\\\&','').replace('\\\\%','')\n",
    "        \n",
    "        temp = [hash(result.title),result.title,result.published,abstract,result.pdf_url]\n",
    "        temp.extend(category_vector)\n",
    "        \n",
    "        temp.append(get_embedding(model,tokenizer,result.title).tolist())\n",
    "        temp.append(get_embedding(model,tokenizer,abstract).tolist())\n",
    "        \n",
    "        paper_data.append(temp)\n",
    "        \n",
    "    df = pd.DataFrame(data=paper_data, columns=['_id','Title','Date', 'Abstract',\"PDF URL\",'Category1','Category2','Category3',\"TitleEmbedding\",\"AbstractEmbedding\"])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_papers_after_date(last_date):\n",
    "    search = arxiv.Search(\n",
    "      query = \"computer science\",\n",
    "      max_results = 1000,\n",
    "      sort_by = arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "    \n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "    model = AutoModel.from_pretrained('allenai/longformer-base-4096')\n",
    "    \n",
    "    paper_data = []\n",
    "    \n",
    "    for result in search.results():\n",
    "        ## To be changed\n",
    "        paper_date = (result.published+timedelta(days=1)).date()\n",
    "        ##\n",
    "        \n",
    "        \n",
    "        if paper_date < last_date.date():\n",
    "            break\n",
    "        \n",
    "        category_vector = [result.categories[i] if i<len(result.categories) else \"Nafin\" for i in range(3)]\n",
    "        \n",
    "        \n",
    "        abstract = result.summary.replace('\\n',' ').replace('\\\\&','').replace('\\\\%','')\n",
    "        \n",
    "        temp = [hash(result.title),result.title,result.published,abstract,result.pdf_url]\n",
    "        temp.extend(category_vector)\n",
    "        \n",
    "        temp.append(get_embedding(model,tokenizer,result.title).tolist())\n",
    "        temp.append(get_embedding(model,tokenizer,abstract).tolist())\n",
    "        \n",
    "        paper_data.append(temp)\n",
    "        \n",
    "    df = pd.DataFrame(data=paper_data, columns=['_id','Title','Date', 'Abstract',\"PDF URL\",'Category1','Category2','Category3',\"TitleEmbedding\",\"AbstractEmbedding\"])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_abstracts_by_ids(paper_index_map):\n",
    "    inv_paper_map = {v: k for k, v in paper_index_map.items()}\n",
    "    \n",
    "    client = MongoClient('mongodb://localhost:27017/?readPreference=primary&appname=MongoDB%20Compass&ssl=false')\n",
    "    \n",
    "    # get all users recently viewed\n",
    "    db = client.papers.papers\n",
    "    \n",
    "    all_viewed = list(db.find({\"_id\":{\"$in\":list(paper_index_map.keys())}},{\"Abstract\":1}))\n",
    "    \n",
    "    data = np.chararray(len(all_viewed),itemsize=10000)\n",
    "    \n",
    "    new_map = {}\n",
    "    data = []\n",
    "    for i in range(len(all_viewed)):\n",
    "        paper_id = all_viewed[i][\"_id\"]\n",
    "        new_map[paper_index_map[paper_id]] = i\n",
    "        data.append(all_viewed[i][\"Abstract\"])\n",
    "    \n",
    "    return np.array(data),new_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     3,
     23
    ]
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ClickedDataset(Dataset):\n",
    "    def __init__(self, abstracts, item_vectors,abstract_map):\n",
    "        self.item_vectors = item_vectors.astype(np.float32)\n",
    "        self.abstracts = abstracts\n",
    "        self.abstract_map = abstract_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.item_vectors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        abstract = self.abstracts[self.abstract_map[idx]]\n",
    "        item_vecs = self.item_vectors[idx]\n",
    "        \n",
    "        sample = (abstract,item_vecs)\n",
    "\n",
    "        return sample    \n",
    "    \n",
    "class paperBERT(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(paperBERT, self).__init__()\n",
    "        self.longformer = AutoModel.from_pretrained('allenai/longformer-base-4096')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "        self.linear1 = nn.Linear(768, 256)\n",
    "        self.linear2 = nn.Linear(256, latent_dim)\n",
    "        \n",
    "    def forward(self,data):\n",
    "        encoding = self.tokenizer.batch_encode_plus(data, return_tensors='pt', padding=True,\n",
    "                                                       truncation=True, add_special_tokens = True).to(device)\n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "        \n",
    "        model_output = self.longformer(\n",
    "               input_ids, \n",
    "               attention_mask=attention_mask)\n",
    "\n",
    "        # sequence_output has the following shape: (batch_size, sequence_length, 768)\n",
    "        sequence_output=  mean_pooling(model_output, attention_mask)\n",
    "        print(sequence_output.shape)\n",
    "        linear1_output = self.linear1(sequence_output.view(-1,768)) ## extract the 1st token's embeddings\n",
    "        linear2_output = self.linear2(linear1_output)\n",
    "\n",
    "        return linear2_output\n",
    "    \n",
    "    def train(self,data_loader,epochs):\n",
    "        criterion = nn.MSELoss() ## If required define your own criterion\n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch in data_loader: ## If you have a DataLoader()  object to get the data.\n",
    "                \n",
    "                data = list(batch[0])\n",
    "                targets = batch[1] ## assuming that data loader returns a tuple of data and its targets\n",
    "                \n",
    "                optimizer.zero_grad()   \n",
    "                outputs = model(data)\n",
    "                outputs = torch.tanh(outputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict(self, data):\n",
    "        with torch.no_grad():\n",
    "            encoding = self.tokenizer.batch_encode_plus(data, return_tensors='pt', padding=True,\n",
    "                                                           truncation=True, add_special_tokens = True).to(device)\n",
    "            input_ids = encoding['input_ids']\n",
    "            attention_mask = encoding['attention_mask']\n",
    "\n",
    "            model_output = self.longformer(\n",
    "                   input_ids, \n",
    "                   attention_mask=attention_mask)\n",
    "\n",
    "            # sequence_output has the following shape: (batch_size, sequence_length, 768)\n",
    "            sequence_output=  mean_pooling(model_output, attention_mask)\n",
    "            print(sequence_output.shape)\n",
    "            linear1_output = self.linear1(sequence_output.view(-1,768)) ## extract the 1st token's embeddings\n",
    "            linear2_output = self.linear2(linear1_output)\n",
    "            outputs = torch.tanh(linear2_output)\n",
    "        return outputs.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start objective: 8320.143828725739\n",
      "Converged: True\n",
      "# iterations: 7\n",
      "Final objective: 3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "paperBERT(\n",
       "  (longformer): LongformerModel(\n",
       "    (embeddings): LongformerEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): LongformerEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): LongformerPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (linear1): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (linear2): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rat,user_index_map,paper_index_map = generate_interaction_matrix()\n",
    "\n",
    "latent_dim = 10\n",
    "MAX_ITERATION = 1000\n",
    "TOLERANCE = 1e-4\n",
    "U,P = approximate(rat, data_mask, latent_dim, 1)\n",
    "\n",
    "abstracts,new_map = get_abstracts_by_ids(paper_index_map)\n",
    "dataset = ClickedDataset(abstracts, P,new_map)\n",
    "data_loader = DataLoader(dataset, batch_size=10,shuffle=True)\n",
    "\n",
    "model = paperBERT(latent_dim) # You can pass the parameters if required to have more flexible model\n",
    "model.to(device) ## can be gpu\n",
    "#model.train(data_loader,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 768])\n"
     ]
    }
   ],
   "source": [
    "item_pred = model.predict(list(abstracts[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.2177798  -0.05733171 -0.10848948 -0.00063107  0.2178803   0.11466382\n",
      "  -0.19518623 -0.04703917  0.24380101 -0.14298704]\n",
      " [-0.20309497 -0.02282331 -0.06286729 -0.05350485  0.19309942  0.10009719\n",
      "  -0.1373236  -0.08971114  0.25474325 -0.15889482]\n",
      " [-0.22715926 -0.0660781  -0.08698583 -0.01901439  0.1952582   0.07186789\n",
      "  -0.14948282 -0.06152859  0.25174576 -0.10831404]\n",
      " [-0.21838889 -0.03443026 -0.10312241 -0.00762006  0.23165008  0.13091125\n",
      "  -0.17319335 -0.09733933  0.26188704 -0.17631511]\n",
      " [-0.20123059 -0.04940609 -0.08939983 -0.022474    0.24247847  0.10977195\n",
      "  -0.17588036 -0.02207063  0.2010569  -0.16957085]\n",
      " [-0.23084119 -0.05561435 -0.09393923  0.01601766  0.19896518  0.1233958\n",
      "  -0.18921125 -0.08285669  0.23452727 -0.13247065]\n",
      " [-0.20250373 -0.09292164 -0.12195797  0.00843132  0.16919988  0.16801114\n",
      "  -0.1658791  -0.11104954  0.2974189  -0.15374184]\n",
      " [-0.23345332 -0.0564669  -0.12840258 -0.01150201  0.14443612  0.1369468\n",
      "  -0.22225943 -0.13190164  0.26721305 -0.16708834]\n",
      " [-0.1738982  -0.07466931 -0.08643615 -0.02373897  0.16660722  0.10820235\n",
      "  -0.23184916 -0.09026107  0.23766032 -0.15872046]\n",
      " [-0.19733441 -0.05865003 -0.12816668 -0.03577138  0.1816887   0.12506062\n",
      "  -0.18796772 -0.10595514  0.27306816 -0.16972336]]\n"
     ]
    }
   ],
   "source": [
    "print(item_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.00229371,  0.00803845,  0.00532361, -0.01641584, -0.00486945,\n",
       "        -0.0027297 ,  0.00100707, -0.02019379,  0.00494651, -0.00947571]),\n",
       " array([-0.23084119, -0.05561435, -0.09393923,  0.01601766,  0.19896518,\n",
       "         0.1233958 , -0.18921125, -0.08285669,  0.23452727, -0.13247065],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[5],item_pred[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rat[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start objective: 185.05454309841716\n",
      "Iteration: 10\n",
      "Current objective: 1.2516780809886026\n",
      "Iteration: 20\n",
      "Current objective: 1.1921378457952398\n",
      "Iteration: 30\n",
      "Current objective: 1.162010818863975\n",
      "Iteration: 40\n",
      "Current objective: 1.1429230464085638\n",
      "Iteration: 50\n",
      "Current objective: 1.1294122630569963\n",
      "Iteration: 60\n",
      "Current objective: 1.1191880473810554\n",
      "Iteration: 70\n",
      "Current objective: 1.1110955862418266\n",
      "Iteration: 80\n",
      "Current objective: 1.1044800347032928\n",
      "Iteration: 90\n",
      "Current objective: 1.0989382654038415\n",
      "Iteration: 100\n",
      "Current objective: 1.0942065211469143\n",
      "Iteration: 110\n",
      "Current objective: 1.0901039104637258\n",
      "Iteration: 120\n",
      "Current objective: 1.0865016270398025\n",
      "Iteration: 130\n",
      "Current objective: 1.0833050967596234\n",
      "Iteration: 140\n",
      "Current objective: 1.08044308449734\n",
      "Iteration: 150\n",
      "Current objective: 1.0778607632734498\n",
      "Iteration: 160\n",
      "Current objective: 1.0755151464572579\n",
      "Iteration: 170\n",
      "Current objective: 1.073371985541284\n",
      "Iteration: 180\n",
      "Current objective: 1.071403607884214\n",
      "Iteration: 190\n",
      "Current objective: 1.0695873751005007\n",
      "Iteration: 200\n",
      "Current objective: 1.0679045618134149\n",
      "Iteration: 210\n",
      "Current objective: 1.0663395255874428\n",
      "Iteration: 220\n",
      "Current objective: 1.0648790826270254\n",
      "Iteration: 230\n",
      "Current objective: 1.0635120315087905\n",
      "Iteration: 240\n",
      "Current objective: 1.0622287851437766\n",
      "Iteration: 250\n",
      "Current objective: 1.0610210830325606\n",
      "Iteration: 260\n",
      "Current objective: 1.0598817638844489\n",
      "Converged: True\n",
      "# iterations: 268\n",
      "Final objective: 1.0589096672882172\n"
     ]
    }
   ],
   "source": [
    "U,P = approximate(rat[:,:10], latent_dim, 1,Y_start=U,Z_start=item_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.00229371,  0.00803845,  0.00532361, -0.01641584, -0.00486945,\n",
       "        -0.0027297 ,  0.00100707, -0.02019379,  0.00494651, -0.00947571],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def loss(X, Y, Z, mask, lam):\n",
    "    return np.sum( ((Y.T @ Z - X) ** 2)[mask] ) \\\n",
    "       + lam * np.linalg.norm(Y, ord=\"fro\") \\\n",
    "       + lam * np.linalg.norm(Z, ord=\"fro\")\n",
    "\n",
    "def approximate(data_dense, rank, lam,max_iteration=1000,Y_start=None,Z_start=None):\n",
    "    # initialize low-rank approximation matrix as a product of two \n",
    "    # Y @ Z = X_bar\n",
    "    data_mask = np.ones(data_dense.shape, dtype=np.bool)\n",
    "    if Y_start is None and Z_start is None:\n",
    "        Y = Y_start\n",
    "        z = Z_start\n",
    "    else:\n",
    "        Y = np.random.randn(rank, data_dense.shape[0])\n",
    "        Z = np.random.randn(rank, data_dense.shape[1])\n",
    "\n",
    "    # calculation speedup\n",
    "    lam_I = lam * np.eye(rank)\n",
    "\n",
    "    # alternating least squares until convergence\n",
    "    prev_obj = loss(data_dense, Y, Z, data_mask, lam)\n",
    "    converged = False\n",
    "\n",
    "    print(\"Start objective:\", prev_obj)\n",
    "\n",
    "    prev_Y = Y.copy()\n",
    "    prev_Z = Z.copy()\n",
    "    for iteration in range(1, max_iteration + 1):\n",
    "        if iteration % 10 == 0:\n",
    "            print(\"Iteration:\", iteration)\n",
    "            print(\"Current objective:\", prev_obj)\n",
    "\n",
    "        # optimize Y based on current value of Z\n",
    "        for col in range(Y.shape[1]):\n",
    "            has_rating = data_mask[col]\n",
    "            ratings = data_dense[col, has_rating]\n",
    "            Z_relevant_columns = Z[:, has_rating]\n",
    "            regularized_cov = np.sum([c.reshape([-1, 1]) @ c.reshape([1, -1]) for\n",
    "                                    c in Z_relevant_columns.T], axis=0) + lam_I\n",
    "            weighted_sum = np.sum(Z_relevant_columns * ratings, axis=1)\n",
    "            Y[:, col] = np.linalg.inv(regularized_cov) @ weighted_sum\n",
    "\n",
    "        # optimize Z based on current values of Y\n",
    "        for col in range(Z.shape[1]):\n",
    "            has_rating = data_mask[:, col]\n",
    "            ratings = data_dense[has_rating, col]\n",
    "            Y_relevant_columns = Y[:, has_rating]\n",
    "            regularized_cov = np.sum([c.reshape([-1, 1]) @ c.reshape([1, -1]) for\n",
    "                                    c in Y_relevant_columns.T], axis=0) + lam_I\n",
    "            weighted_sum = np.sum(Y_relevant_columns * ratings, axis=1)\n",
    "            Z[:, col] = np.linalg.inv(regularized_cov) @ weighted_sum\n",
    "\n",
    "        obj = loss(data_dense, Y, Z, data_mask, lam)\n",
    "\n",
    "        # convergence criteria. prevents division by 0.\n",
    "        if abs(obj - prev_obj) / ( abs(prev_obj) + 1e-8 ) < TOLERANCE:\n",
    "            converged = True\n",
    "            break\n",
    "\n",
    "        prev_obj = obj\n",
    "\n",
    "    print(\"Converged:\", converged)\n",
    "    print(\"# iterations:\", iteration)\n",
    "    print(\"Final objective:\", obj)\n",
    "\n",
    "    # report final low-rank matrix\n",
    "    return Y.T, Z.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def generate_interaction_matrix():\n",
    "    client = MongoClient('mongodb://localhost:27017/?readPreference=primary&appname=MongoDB%20Compass&ssl=false')\n",
    "    \n",
    "    # get all users recently viewed\n",
    "    db = client.papers.users\n",
    "    \n",
    "    all_viewed = db.find({},{\"recently_viewed\":1})\n",
    "    users_viewed = list(all_viewed)\n",
    "    \n",
    "    # get all papers and their ids\n",
    "    db = client.papers.papers\n",
    "    \n",
    "    all_paper_ids = db.find({},{\"_id\":1})\n",
    "    all_ids = list(all_paper_ids)\n",
    "    \n",
    "    # make dictionary between paper id and index in array\n",
    "    paper_index_map = {}\n",
    "    \n",
    "    i=0\n",
    "    for dic in all_ids:\n",
    "        paper_index_map[dic[\"_id\"]] = i\n",
    "        i+=1\n",
    "    \n",
    "    # make array of zeroes with shape (users,papers)\n",
    "    clicked = np.zeros((len(users_viewed),len(all_ids)))\n",
    "    \n",
    "    # for all users recently viewed change 0 in array to a 1\n",
    "    user_index_map = {}\n",
    "    i=0\n",
    "    for user in users_viewed:\n",
    "        user_index_map[user[\"_id\"]] = i\n",
    "        \n",
    "        for paper in user[\"recently_viewed\"]:\n",
    "            clicked[i,paper_index_map[paper]] = 1\n",
    "        \n",
    "        i+=1\n",
    "    \n",
    "    return clicked,user_index_map, paper_index_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_two_weeks():\n",
    "    client = MongoClient('mongodb://localhost:27017/?readPreference=primary&appname=MongoDB%20Compass&ssl=false')\n",
    "\n",
    "    db = client.papers.users\n",
    "    user = db.find_one({'_id':0})\n",
    "    \n",
    "    # Get paperst hat had been missed\n",
    "    df = get_papers_after_date(datetime.now()-timedelta(days=14))\n",
    "    \n",
    "    # Add them to the database\n",
    "    db=client.papers.papers\n",
    "    db.insert_many(df.to_dict('records'))\n",
    "    \n",
    "    # Remove old papers\n",
    "    db.delete_many({'Last_date':{'$lt' : last_date_updated-timedelta(days=14)}})\n",
    "    \n",
    "def get_last_days_papers():\n",
    "    client = MongoClient('mongodb://localhost:27017/?readPreference=primary&appname=MongoDB%20Compass&ssl=false')\n",
    "\n",
    "    # Get paperst hat had been missed\n",
    "    df = get_papers_after_date(datetime.now()-timedelta(days=1))\n",
    "    \n",
    "    # Add them to the database\n",
    "    db=client.papers.papers\n",
    "    db.insert_many(df.to_dict('records'))\n",
    "    \n",
    "    # Remove old papers\n",
    "    db.delete_many({'Last_date':{'$lt' : last_date_updated-timedelta(days=14)}})\n",
    "    \n",
    "def dummy_user():\n",
    "    client = MongoClient('mongodb://localhost:27017/?readPreference=primary&appname=MongoDB%20Compass&ssl=false')\n",
    "    db = client.papers.users\n",
    "    db.insert_one({\"_id\":0,\"last_date_updated\":datetime.now(),\"recently_viewed\":[],\n",
    "                   \"papers_read\":0,\"average_title\":[[0 for i in range(768)]],\"average_abstract\":[[0 for i in range(768)]],\n",
    "                   \"title_weights\":[.5]})\n",
    "    \n",
    "def delete_docs():\n",
    "    client = MongoClient('mongodb://localhost:27017/?readPreference=primary&appname=MongoDB%20Compass&ssl=false')\n",
    "    db = client.papers.papers\n",
    "    db.delete_many({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'last_date_updated' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-42c4d2d17778>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdelete_docs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mget_last_two_weeks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-54-d3bbacec666d>\u001b[0m in \u001b[0;36mget_last_two_weeks\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# Remove old papers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete_many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Last_date'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'$lt'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mlast_date_updated\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_last_days_papers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'last_date_updated' is not defined"
     ]
    }
   ],
   "source": [
    "delete_docs()\n",
    "get_last_two_weeks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo\n",
    "Now i gotta start having an interaction with front end and backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(query,max_results):\n",
    "    search = arxiv.Search(\n",
    "      query = query,\n",
    "      max_results = max_results,\n",
    "      sort_by = arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "    \n",
    "    paper_data = []\n",
    "    \n",
    "    for result in search.results():\n",
    "        category_vector = [result.categories[i] if i<len(result.categories) else \"Nafin\" for i in range(3)]\n",
    "        \n",
    "        \n",
    "        abstract = result.summary.replace('\\n',' ').replace('\\\\&','').replace('\\\\%','')\n",
    "        \n",
    "        temp = [result.title,result.published,abstract,result.pdf_url]\n",
    "        temp.extend(category_vector)\n",
    "        paper_data.append(temp)\n",
    "        \n",
    "    df = pd.DataFrame(data=paper_data, columns=['Title','Date', 'Abstract',\"PDF URL\",'Category1','Category2','Category3'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def get_embeddings(df):\n",
    "    model = AutoModel.from_pretrained('allenai/longformer-base-4096')\n",
    "    tokenizer = AutoTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "    \n",
    "    abstracts = df.Abstract.values\n",
    "    abstracts = [str(a) for a in abstracts]\n",
    "    \n",
    "    encoded_input = tokenizer(abstracts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    \n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    \n",
    "    return sentence_embeddings.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(\"computer science\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "embeddings = get_embeddings(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Close Papers: \n",
      "\n",
      "['ANCER: Anisotropic Certification via Sample-wise Volume Maximization'\n",
      " 'Score refinement for confidence-based 3D multi-object tracking'\n",
      " 'Gradient-Based Quantification of Epistemic Uncertainty for Deep Object Detectors'\n",
      " 'ViTGAN: Training GANs with Vision Transformers'\n",
      " 'Deep Learning for Reduced Order Modelling and Efficient Temporal Evolution of Fluid Simulations']\n",
      " (-1-Exit)-1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import BallTree\n",
    "kdt = BallTree(embeddings)\n",
    "dist, positions = kdt.query([embeddings[col==1].mean(axis=0)],k=5)\n",
    "\n",
    "\n",
    "for p in positions:\n",
    "    print(\"Close Papers: \")\n",
    "    print()\n",
    "\n",
    "    close_titles = data.Title.values[p]\n",
    "    print(close_titles)\n",
    "\n",
    "    val = input(\" (-1-Exit)\")\n",
    "    if val == \"-1\":\n",
    "        break\n",
    "\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1df7d0e2c88>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(result[:,0],result[:,1],c=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightedL2(a,b):\n",
    "    q = a-b\n",
    "    return np.sqrt((weights*q*q).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_closest_papers(df,embeddings,neighbors):\n",
    "    titles,abstracts = df.Title.values, df.Abstract.values\n",
    "    knt = KNeighborsTransformer(n_neighbors=neighbors)\n",
    "    knt.fit(embeddings)\n",
    "    A = knt.kneighbors_graph(embeddings).toarray()\n",
    "    \n",
    "    \n",
    "    for i in range(df.shape[0]):\n",
    "        print(\"Paper to compare: \", titles[i])\n",
    "        print()\n",
    "        print(abstracts[i])\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        print(\"Other Papers: \")\n",
    "        print()\n",
    "        \n",
    "        v = A[i]\n",
    "        close_titles = data.Title.values[(v==1)]\n",
    "        print(close_titles)\n",
    "        \n",
    "        val = input(\" (-1-Exit)\")\n",
    "        if val == \"-1\":\n",
    "            break\n",
    "\n",
    "        clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=12)\n",
    "pca_embeddings = pca.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper to compare:  Representation Consolidation for Training Expert Students\n",
      "\n",
      "Traditionally, distillation has been used to train a student model to emulate the input/output functionality of a teacher. A more useful goal than emulation, yet under-explored, is for the student to learn feature representations that transfer well to future tasks. However, we observe that standard distillation of task-specific teachers actually *reduces* the transferability of student representations to downstream tasks. We show that a multi-head, multi-task distillation method using an unlabeled proxy dataset and a generalist teacher is sufficient to consolidate representations from task-specific teacher(s) and improve downstream performance, outperforming the teacher(s) and the strong baseline of ImageNet pretrained features. Our method can also combine the representational knowledge of multiple teachers trained on one or multiple domains into a single model, whose representation is improved on all teachers' domain(s).\n",
      "\n",
      "Other Papers: \n",
      "\n",
      "['Representation Consolidation for Training Expert Students'\n",
      " 'Controlled AutoEncoders to Generate Faces from Voices'\n",
      " 'Learning Locomotion Controllers for Walking Using Deep FBSDE'\n",
      " 'Unsupervised Discovery of Object Radiance Fields'\n",
      " 'Multiple Instance Learning with Auxiliary Task Weighting for Multiple Myeloma Classification']\n",
      " (-1-Exit)-1\n"
     ]
    }
   ],
   "source": [
    "show_closest_papers(data,pca_embeddings,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.random.uniform(size=(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8195742 , 0.62438009, 0.6366399 , 0.06499433, 0.19564744,\n",
       "       0.52409539, 0.36947022, 0.97009038, 0.84978595, 0.87923217,\n",
       "       0.36252663, 0.55420074])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curate_results_beta(embeddings,average_title,average_abstract,title_weights,num_papers):\n",
    "    ids = [e[\"_id\"] for e in embeddings]\n",
    "    title_embeddings = [e[\"TitleEmbedding\"][0] for e in embeddings]\n",
    "    abstract_embeddings = [e[\"AbstractEmbedding\"][0] for e in embeddings]\n",
    "    \n",
    "    ## Get closest title embeddings\n",
    "    title_tree = BallTree(np.array(title_embeddings))\n",
    "    title_dist, title_positions = title_tree.query(average_title,k=100)\n",
    "    title_dist, title_positions = title_dist[0].tolist(), title_positions[0].tolist()\n",
    "    \n",
    "    ## Get closest abstract embeddings\n",
    "    abstract_tree = BallTree(np.array(abstract_embeddings))\n",
    "    abstract_dist, abstract_positions = abstract_tree.query(average_abstract,k=100)\n",
    "    abstract_dist, abstract_positions = abstract_dist[0].tolist(), abstract_positions[0].tolist()\n",
    "    \n",
    "    # Get random weights\n",
    "    if num_papers > 10:\n",
    "        bandwith = (1/(len(title_weights)))/10\n",
    "        kde = KernelDensity(bandwidth=bandwith).fit(title_weights)\n",
    "        sampled_title_weights = kde.sample(len(title_embeddings))\n",
    "        sampled_title_weights[sampled_title_weights<0] = 0\n",
    "        sampled_title_weights[sampled_title_weights>1] = 1\n",
    "    else:\n",
    "        sampled_title_weights = np.full((len(title_embeddings),1),.5)\n",
    "    \n",
    "    ## Compute distances\n",
    "    combined = {}\n",
    "    weights = {}\n",
    "    for i in range(100):\n",
    "        # add title vector\n",
    "        e = title_positions[i]\n",
    "        weights[ids[e]] = sampled_title_weights[i]\n",
    "        if e not in combined:\n",
    "            combined[ids[e]] = title_dist[i]*sampled_title_weights[i]\n",
    "        else:\n",
    "            combined[ids[e]] += title_dist[i]\n",
    "            \n",
    "        # add abstract vector\n",
    "        e = abstract_positions[i]\n",
    "        \n",
    "        if ids[e] not in weights:\n",
    "            weights[ids[e]] = sampled_title_weights[i]\n",
    "        \n",
    "        if e not in combined:\n",
    "            combined[ids[e]] = abstract_dist[i]*(1-sampled_title_weights[i])\n",
    "        else:\n",
    "            combined[ids[e]] += abstract_dist[i]*(1-sampled_title_weights[i])\n",
    "    \n",
    "    ## Sort by lowest distance\n",
    "    combined_dist = [(k,v) for k,v in combined.items()]\n",
    "    combined_dist = sorted(combined_dist,key=lambda t: t[1])\n",
    "    return [c[0] for c in combined_dist], weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_popular_papers():\n",
    "    client = MongoClient('mongodb://localhost:27017/?readPreference=primary&appname=MongoDB%20Compass&ssl=false')\n",
    "\n",
    "    db = client.papers.papers\n",
    "    papers = db.find({\"date\" : {\"$gt\": datetime.now()-timedelta(days=2)}},{\"title\":1,\n",
    "                                                                          \"abstract\":1,\n",
    "                                                                          \"date\":1,\n",
    "                                                                          \"number_of_clicks\":1})\n",
    "    \n",
    "    paper_list = []\n",
    "    \n",
    "    for p in list(papers):\n",
    "        paper_list.append((p[\"_id\"],p[\"number_of_clicks\"]))\n",
    "        \n",
    "    paper_list = sorted(paper_list, key=lambda t: t[1],reverse=True)\n",
    "    print([t for t in paper_list[4]])\n",
    "    db.find({\"_id\": {\"$in\" : [t[0] for t in paper_list[4]]}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Int64' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-274005f1095e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_popular_papers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-73-78d3e8d65a4d>\u001b[0m in \u001b[0;36mget_popular_papers\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mpaper_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpaper_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpaper_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"_id\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"$in\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpaper_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-73-78d3e8d65a4d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mpaper_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpaper_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpaper_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"_id\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"$in\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpaper_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Int64' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "get_popular_papers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
